{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis, Offensive language and emotion recognition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline, AutoConfig\n",
    "import torch\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative', 'neutral', 'positive']\n"
     ]
    }
   ],
   "source": [
    "# labels retrival\n",
    "labels_sentiment=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels_sentiment = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "print(labels_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Users\\Castagna\\anaconda3\\envs\\myenv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Classification throught sentiment analysis in 3 labels: 'Negative', 'Neutral' or 'Positive'\n",
    "sentiment_analysis_model = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "model_sentiment = AutoModelForSequenceClassification.from_pretrained(sentiment_analysis_model)\n",
    "tokenizer_sentiment = AutoTokenizer.from_pretrained(sentiment_analysis_model)\n",
    "\n",
    "def sentiment_analysis(text, model, tokenizer, lables):\n",
    "  labels = ['Negative', 'Neutral', 'Positive']\n",
    "  # Padding true for allow the model to have the same dimension for all the data that we pass. it will automatically use mask value as 0 for text with padding (so will not be considered).\n",
    "  # 'pt' means a pytorch output\n",
    "  encoded_tweet = tokenizer(text, max_length=512, padding='max_length', truncation=True, return_tensors='pt')\n",
    "  # It doesn't update the gradients and so the computational time decrease\n",
    "  with torch.no_grad():\n",
    "    output = model(**encoded_tweet)\n",
    "  scores = output[0][0].detach().numpy()\n",
    "  scores = softmax(scores)\n",
    "  max_index = scores.argmax()\n",
    "  max_score = scores[max_index]\n",
    "  max_label = labels_sentiment[max_index]\n",
    "\n",
    "  return max_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentiment_analysis('i am very happy to be here', model_sentiment, tokenizer_sentiment, labels_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'joy', 'optimism', 'sadness']\n"
     ]
    }
   ],
   "source": [
    "# labels retrival\n",
    "labels_emotion=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels_emotion = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "print(labels_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='emotion'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "model_emotion = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "tokenizer_emotion = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "def emotion_recognition(text, model, tokenizer, labels):\n",
    "  # Padding true for allow the model to have the same dimension for all the data that we pass. it will automatically use mask value as 0 for text with padding (so will not be considered).\n",
    "  # 'pt' means a pytorch output\n",
    "  # truncated means that, based on how the model has been trained, if a sentence is longer than the maximum handable from the model, the sentence will be truncated.\n",
    "  encoded_tweet = tokenizer(text, max_length=512, padding='max_length', truncation=True, return_tensors='pt')\n",
    "\n",
    "  # The output is a unnormalized score that needs to be pass to a softmax function for asses the exact percentage\n",
    "  with torch.no_grad():\n",
    "    output = model(**encoded_tweet)\n",
    "  scores = output[0][0].detach().numpy()\n",
    "  scores = softmax(scores)\n",
    "  max_index = scores.argmax()\n",
    "  max_score = scores[max_index]\n",
    "  max_label = labels[max_index]\n",
    "\n",
    "  return max_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offensive language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not-offensive', 'offensive']\n"
     ]
    }
   ],
   "source": [
    "# label retrival\n",
    "labels_offensive=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels_offensive = [row[1] for row in csvreader if len(row) > 1]\n",
    "print(labels_offensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='offensive'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "model_offensive = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "tokenizer_offensive = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "def offensive_language(text, model, tokenizer, labels):\n",
    "  # Padding true for allow the model to have the same dimension for all the data that we pass. it will automatically use mask value as 0 for text with padding (so will not be considered).\n",
    "  # 'pt' means a pytorch output\n",
    "  encoded_tweet = tokenizer(text, max_length=512, padding='max_length', truncation=True, return_tensors='pt')\n",
    "\n",
    "  # The output is a unnormalized score that needs to be pass to a softmax function for asses the exact percentage\n",
    "  with torch.no_grad():\n",
    "    output = model(**encoded_tweet)\n",
    "  scores = output[0][0].detach().numpy()\n",
    "  scores = softmax(scores)\n",
    "  max_index = scores.argmax()\n",
    "  max_score = scores[max_index]\n",
    "  max_label = labels[max_index]\n",
    "\n",
    "  return max_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files estratti:\n",
      "['r_ethtrader_comments_2016.jsonl', 'r_ethtrader_comments_2017.jsonl', 'r_ethtrader_comments_2018.jsonl', 'r_ethtrader_comments_2018_bkp.jsonl', 'r_ethtrader_comments_2019.jsonl', 'r_ethtrader_comments_2020.jsonl', 'r_ethtrader_comments_2021.jsonl', 'r_ethtrader_comments_2022.jsonl', 'r_ethtrader_comments_2023.jsonl', 'r_ethtrader_posts_2016.jsonl', 'r_ethtrader_posts_2017.jsonl', 'r_ethtrader_posts_2018.jsonl', 'r_ethtrader_posts_2019.jsonl', 'r_ethtrader_posts_2020.jsonl', 'r_ethtrader_posts_2021.jsonl', 'r_ethtrader_posts_2022.jsonl', 'r_ethtrader_posts_2023.jsonl']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['r_ethtrader_posts_2016.jsonl',\n",
       " 'r_ethtrader_posts_2017.jsonl',\n",
       " 'r_ethtrader_posts_2018.jsonl',\n",
       " 'r_ethtrader_posts_2019.jsonl',\n",
       " 'r_ethtrader_posts_2020.jsonl',\n",
       " 'r_ethtrader_posts_2021.jsonl',\n",
       " 'r_ethtrader_posts_2022.jsonl',\n",
       " 'r_ethtrader_posts_2023.jsonl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "\n",
    "file_zip = 'insert path'       #\"C:\\\\Users\\\\aless\\\\Desktop\\\\reddit_distr\\\\posts\"\n",
    "\n",
    "os.chdir(file_zip)\n",
    "\n",
    "print(\"Files estratti:\")\n",
    "print(os.listdir())\n",
    "rewards=sorted(os.listdir())\n",
    "rewards=rewards[9:]\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r_ethtrader_posts_2016.jsonl\n",
      "r_ethtrader_posts_2017.jsonl\n",
      "r_ethtrader_posts_2018.jsonl\n",
      "r_ethtrader_posts_2019.jsonl\n",
      "r_ethtrader_posts_2020.jsonl\n",
      "r_ethtrader_posts_2021.jsonl\n",
      "r_ethtrader_posts_2022.jsonl\n",
      "r_ethtrader_posts_2023.jsonl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Text</th>\n",
       "      <th>Up-Down</th>\n",
       "      <th>Year</th>\n",
       "      <th>retrieved_utc</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>Retrieved_on</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>carlslarson</td>\n",
       "      <td>How would a switch to Proof of Stake affect th...</td>\n",
       "      <td>10</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1427296704</td>\n",
       "      <td>1.440833e+09</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heliumcraft</td>\n",
       "      <td>Price predictions?What do you think will be th...</td>\n",
       "      <td>12</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1427369350</td>\n",
       "      <td>1.440832e+09</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>StonedSheep</td>\n",
       "      <td>Exchange?Which exchanges will Ethereum first s...</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1427582991</td>\n",
       "      <td>1.440827e+09</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heliumcraft</td>\n",
       "      <td>What software do you use for trading?</td>\n",
       "      <td>3</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1428015859</td>\n",
       "      <td>1.440817e+09</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>carlslarson</td>\n",
       "      <td>What factors will influence the price of Ether?</td>\n",
       "      <td>9</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1430401513</td>\n",
       "      <td>1.440763e+09</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365557</th>\n",
       "      <td>MasterpieceLoud4931</td>\n",
       "      <td>In 2023, the US government tried to kill crypto</td>\n",
       "      <td>7</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1704047821</td>\n",
       "      <td>1.704048e+09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365558</th>\n",
       "      <td>Prog132487</td>\n",
       "      <td>Biggest Crypto and NFT Games of 2023 - Decrypt</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1704051863</td>\n",
       "      <td>1.704052e+09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365559</th>\n",
       "      <td>teeceaustralia</td>\n",
       "      <td>Vitalik Buterin Reveals Ethereum’s Road Map Fo...</td>\n",
       "      <td>7</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1704059279</td>\n",
       "      <td>1.704059e+09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365560</th>\n",
       "      <td>aItalianStallion</td>\n",
       "      <td>Chainlink News: CCIP, Data Feeds &amp; Streams, Pr...</td>\n",
       "      <td>0</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1704064260</td>\n",
       "      <td>1.704064e+09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365561</th>\n",
       "      <td>MadKingAegon</td>\n",
       "      <td>Unlocking the Future: Vone - Where Privacy Mee...</td>\n",
       "      <td>20</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1704065963</td>\n",
       "      <td>1.704066e+09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>314015 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Author  \\\n",
       "0               carlslarson   \n",
       "1               heliumcraft   \n",
       "2               StonedSheep   \n",
       "4               heliumcraft   \n",
       "5               carlslarson   \n",
       "...                     ...   \n",
       "365557  MasterpieceLoud4931   \n",
       "365558           Prog132487   \n",
       "365559       teeceaustralia   \n",
       "365560     aItalianStallion   \n",
       "365561         MadKingAegon   \n",
       "\n",
       "                                                     Text  Up-Down  Year  \\\n",
       "0       How would a switch to Proof of Stake affect th...       10  2016   \n",
       "1       Price predictions?What do you think will be th...       12  2016   \n",
       "2       Exchange?Which exchanges will Ethereum first s...        5  2016   \n",
       "4                   What software do you use for trading?        3  2016   \n",
       "5         What factors will influence the price of Ether?        9  2016   \n",
       "...                                                   ...      ...   ...   \n",
       "365557    In 2023, the US government tried to kill crypto        7  2023   \n",
       "365558     Biggest Crypto and NFT Games of 2023 - Decrypt        1  2023   \n",
       "365559  Vitalik Buterin Reveals Ethereum’s Road Map Fo...        7  2023   \n",
       "365560  Chainlink News: CCIP, Data Feeds & Streams, Pr...        0  2023   \n",
       "365561  Unlocking the Future: Vone - Where Privacy Mee...       20  2023   \n",
       "\n",
       "        retrieved_utc  created_utc  Retrieved_on  num_comments  \n",
       "0                 NaN   1427296704  1.440833e+09             9  \n",
       "1                 NaN   1427369350  1.440832e+09            27  \n",
       "2                 NaN   1427582991  1.440827e+09             7  \n",
       "4                 NaN   1428015859  1.440817e+09             2  \n",
       "5                 NaN   1430401513  1.440763e+09            11  \n",
       "...               ...          ...           ...           ...  \n",
       "365557            NaN   1704047821  1.704048e+09             1  \n",
       "365558            NaN   1704051863  1.704052e+09             1  \n",
       "365559            NaN   1704059279  1.704059e+09             1  \n",
       "365560            NaN   1704064260  1.704064e+09             1  \n",
       "365561            NaN   1704065963  1.704066e+09             1  \n",
       "\n",
       "[314015 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_file = rewards\n",
    "all_data=pd.DataFrame()\n",
    "for i in selected_file:\n",
    "  print(i)\n",
    "  author=[]\n",
    "  text=[]\n",
    "  up_down=[]\n",
    "  year=[]\n",
    "  retrieved_on=[]\n",
    "  retrieved_utc=[]\n",
    "  created_utc=[]\n",
    "  link_flair_text=[]\n",
    "  num_comments=[]\n",
    "  with open(i, 'r', encoding='UTF-8') as f:\n",
    "      for line in f:\n",
    "        line=json.loads(line)\n",
    "        author.append(line['author'])\n",
    "        all_text=line['title'] + line['selftext']\n",
    "        text.append(all_text.replace('\\n', ' '))\n",
    "        up_down.append(line['ups'])\n",
    "        retrieved_on.append(line.get('retrieved_on', np.nan))\n",
    "        retrieved_utc.append(line.get('retrieved_utc', np.nan))\n",
    "        created_utc.append(line.get('created_utc', np.nan))\n",
    "        link_flair_text.append(line.get('link_flair_text', np.nan))\n",
    "        num_comments.append(line.get('num_comments', np.nan))\n",
    "        if '2016' in i:\n",
    "          year.append('2016')\n",
    "        elif '2017' in i:\n",
    "          year.append('2017')\n",
    "        elif '2018' in i:\n",
    "          year.append('2018')\n",
    "        elif '2019' in i:\n",
    "          year.append('2019')\n",
    "        elif '2020' in i:\n",
    "          year.append('2020')\n",
    "        elif '2021' in i:\n",
    "          year.append('2021')\n",
    "        elif '2022' in i:\n",
    "          year.append('2022')\n",
    "        elif '2023' in i:\n",
    "          year.append('2023')\n",
    "          \n",
    "\n",
    "  data = pd.DataFrame({\n",
    "      'Author': author,\n",
    "      'Text': text,\n",
    "      'Up-Down': up_down,\n",
    "      'Year': year,\n",
    "      'retrieved_utc': retrieved_utc,\n",
    "      'created_utc': created_utc,\n",
    "      'Retrieved_on': retrieved_on,\n",
    "      'num_comments': num_comments\n",
    "    })\n",
    "  all_data = pd.concat([all_data, data], ignore_index=True)\n",
    "\n",
    "all_data=all_data[all_data['Author']!='[deleted]']\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalization(text):\n",
    "  result = []\n",
    "  for word in text.split(' '):\n",
    "      if word.startswith('@') and len(word) > 1:\n",
    "          word = '@user'\n",
    "      elif word.startswith('http'):\n",
    "          word = \"http\"\n",
    "      elif word.startswith('www.'):\n",
    "          word = \"www\"\n",
    "      result.append(word)\n",
    "  result = \" \".join(result)\n",
    "  return  result\n",
    "all_data['Text'] = all_data['Text'].astype(str).apply(lambda x: generalization(x))\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_entry(x):\n",
    "    return pd.Series({\n",
    "        'sentiment': sentiment_analysis(x, model_sentiment, tokenizer_sentiment, labels_sentiment),\n",
    "        'emotion': emotion_recognition(x, model_emotion, tokenizer_emotion, labels_emotion),\n",
    "        'offensive': offensive_language(x, model_offensive, tokenizer_offensive, labels_offensive),\n",
    "        'distilbert': distilbert(x),\n",
    "        'ernie': ernie(x)\n",
    "    })\n",
    "\n",
    "batch_size = 500\n",
    "start_time = time.time()\n",
    "\n",
    "for start in range(0, len(genarlized_text['pre_processed_text']), batch_size):\n",
    "    end = start + batch_size\n",
    "    results_df = genarlized_text['pre_processed_text'][start:end].apply(process_entry)\n",
    "\n",
    "    mode = 'w' if start == 0 else 'a'\n",
    "    header = True if start == 0 else False\n",
    "\n",
    "    results_df.to_csv('C:\\\\Users\\\\Castagna\\\\Desktop\\\\sentiment_emotion_offensive.csv', mode=mode, header=header, index=False)\n",
    "\n",
    "    tempo_trascorso = time.time() - start_time\n",
    "    print(f\"Processed batch from {start} to {end}, Tempo trascorso: {tempo_trascorso / 60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7660be3ec7bd40fe9786c838dee20aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import TextClassificationPipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "model_reward = AutoModel.from_pretrained(\n",
    "    \"internlm/internlm2-7b-reward\", \n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenize_reward= AutoTokenizer.from_pretrained(\"internlm/internlm2-7b-reward\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_crypto_bert = AutoTokenizer.from_pretrained(\"ElKulako/cryptobert\", use_fast=True)\n",
    "model_crypto_bert= AutoModelForSequenceClassification.from_pretrained(\"ElKulako/cryptobert\", num_labels = 3)\n",
    "pipe = TextClassificationPipeline(model=model_crypto_bert, tokenizer=tokenizer_crypto_bert, max_length=64, truncation=True, padding = 'max_length')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_data['input_reward_model']=[[{\"role\": \"user\", \"content\": i}] for i in all_merged_first['Full Text'] ]\n",
    "\n",
    "reward_model_df= pd.DataFrame()\n",
    "crypto_bert_df= pd.DataFrame()\n",
    "crypto_reward=pd.DataFrame()\n",
    "\n",
    "def reward(x):\n",
    "    return pd.Series({\n",
    "        'reward_model': model_reward.get_score(tokenize_reward, x)\n",
    "    })\n",
    "\n",
    "def process_entry(x):\n",
    "    return pd.Series({\n",
    "        'crpto_bert': pipe(x)\n",
    "\n",
    "    })\n",
    "\n",
    "batch_size = 200\n",
    "start_time = time.time()\n",
    "\n",
    "for start in range(0, len(all_data['input_reward_model']), batch_size):\n",
    "    end = start + batch_size\n",
    "    reward_model_df = all_data['input_reward_model'][start:end].apply(reward)\n",
    "    crypto_bert_df = all_data['Full Text'][start:end].apply(process_entry)\n",
    "    unione = pd.concat([reward_model_df, crypto_bert_df], axis=0)\n",
    "\n",
    "    crypto_reward = pd.concat([crypto_reward, unione], ignore_index=True)\n",
    "\n",
    "    mode = 'w' if start == 0 else 'a'\n",
    "    header = True if start == 0 else False\n",
    "    crypto_reward.to_csv('C:\\\\Users\\\\Castagna\\\\Desktop\\\\reward_cryptoBert.csv', mode='a', header=False, index=False)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Processed batch from {start} to {end}, Tempo trascorso: {elapsed_time / 60:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dummies = pd.get_dummies(results_df['sentiment'], prefix='sentiment').astype(int)\n",
    "emotion_dummies = pd.get_dummies(results_df['emotion'], prefix='emotion').astype(int)\n",
    "offensive_dummies = pd.get_dummies(results_df['offensive'], prefix='offensive').astype(int)\n",
    "\n",
    "encoded_emo_sent_off = pd.concat([\n",
    "    sentiment_dummies,\n",
    "    emotion_dummies,\n",
    "    offensive_dummies\n",
    "], axis=1)\n",
    "\n",
    "\n",
    "encoded_emo_sent_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import textstat\n",
    "import nltk\n",
    "import re\n",
    "import emoji\n",
    "import spacy\n",
    "\n",
    "nltk.download('punkt')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def calculate_readability_metrics(text):\n",
    "    metrics = {}\n",
    "    metrics['Lix'] = textstat.lix(text)\n",
    "    metrics['Gunning Fog'] = textstat.gunning_fog(text)\n",
    "    metrics['ARI'] = textstat.automated_readability_index(text)\n",
    "    metrics['Coleman-Liau'] = textstat.coleman_liau_index(text)\n",
    "    metrics['SMOG'] = textstat.smog_index(text)\n",
    "    metrics['crypto_bert'] = pipe(text)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Function to count emojis\n",
    "def count_emojis(text):\n",
    "    return len([char for char in text if char in emoji.EMOJI_DATA])\n",
    "\n",
    "# Function to count links\n",
    "def count_links(text):\n",
    "    if isinstance(text, str):\n",
    "        url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        return len(url_pattern.findall(text))\n",
    "    return 0\n",
    "\n",
    "# Function to calculate text statistics\n",
    "def analyze_text(text):\n",
    "    metrics = {}\n",
    "    metrics['emoji_count'] = count_emojis(text)\n",
    "    metrics['link_count'] = count_links(text)\n",
    "    metrics['post_length'] = len(text)\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    metrics['word_count'] = len([token.text for token in doc if token.is_alpha])\n",
    "    metrics['sentence_count'] = len(list(doc.sents))\n",
    "    \n",
    "    metrics['syllable_count'] = sum(count_syllables(token.text) for token in doc if token.is_alpha)\n",
    "    metrics['avg_sentence_length'] = metrics['word_count'] / metrics['sentence_count'] if metrics['sentence_count'] > 0 else 0\n",
    "    metrics['avg_syllables_per_word'] = metrics['syllable_count'] / metrics['word_count'] if metrics['word_count'] > 0 else 0\n",
    "    \n",
    "    # Flesch Reading Ease\n",
    "    metrics['reading_ease'] = 206.835 - (1.015 * metrics['avg_sentence_length']) - (84.6 * metrics['avg_syllables_per_word'])\n",
    "    # Flesch-Kincaid Grade Level\n",
    "    metrics['grade_level'] = 0.39 * metrics['avg_sentence_length'] + 11.8 * metrics['avg_syllables_per_word'] - 15.59\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Combine the two analyses\n",
    "def analyze_full_text(text):\n",
    "    readability_metrics = calculate_readability_metrics(text)\n",
    "    text_metrics = analyze_text(text)\n",
    "    \n",
    "    # Combine both dictionaries\n",
    "    combined_metrics = {**readability_metrics, **text_metrics}\n",
    "    return combined_metrics\n",
    "\n",
    "combined_metrics = all_data['pre_processed_text'].apply(analyze_full_text)\n",
    "final_metrics_df = pd.DataFrame(list(combined_metrics))\n",
    "final_metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bearish=[]\n",
    "Neutral=[]\n",
    "Bullish=[]\n",
    "\n",
    "for i in crypto_reward['crypto_bert']:\n",
    "    if 'Neutral' in str(i):\n",
    "        Neutral.append(1)\n",
    "    else:\n",
    "        Neutral.append(0)\n",
    "\n",
    "    if 'Bullish' in str(i):\n",
    "        Bullish.append(1)\n",
    "    else:\n",
    "        Bullish.append(0)\n",
    "\n",
    "    if 'Bearish' in str(i):\n",
    "        Bearish.append(1)\n",
    "    else:\n",
    "        Bearish.append(0)\n",
    "\n",
    "final_metrics_df.drop('crypto_bert', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "final_metrics_df['Neutral']=Neutral\n",
    "final_metrics_df['Bearish']=Bearish\n",
    "final_metrics_df['Bullish']=Bullish\n",
    "final_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista=[\"Other label or No label\",\"Adoption\",\"Altcoin\",\"Announcement\",\"Comedy\",\"Dapp\",\"Discussion\",\"Educational\",\"Exchange\",\"Fundamentals\",\"Media\",\"Metrics\",\"Mining-Staking\",\"News\",\"Security\",\"Sentiment\",\"Strategy\",\"Support\",\"Technicals\",\"Tool\",\"Trading\",\"Warning\"]\n",
    "lista_lower = [label.lower() for label in lista]\n",
    "\n",
    "all_data['link_flair_text'] = all_data['link_flair_text'].str.lower()\n",
    "\n",
    "all_data['link_flair_text'] = all_data['link_flair_text'].apply(\n",
    "    lambda x: x if x in lista_lower else 'no label'\n",
    ")\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = pd.get_dummies(all_data['link_flair_text'], prefix='flair')\n",
    "link = link.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.reset_index(drop=True, inplace=True)\n",
    "link.reset_index(drop=True, inplace=True)\n",
    "final_metrics_df.reset_index(drop=True, inplace=True)\n",
    "encoded_emo_sent_off.reset_index(drop=True, inplace=True)\n",
    "\n",
    "total_data = pd.concat([\n",
    "    all_data.drop(columns=['Author', 'Full Text', 'Year','link_flair_text', 'retrieved_utc', 'Retrieved_on'] ),\n",
    "link,\n",
    "final_metrics_df,\n",
    "encoded_emo_sent_off\n",
    "], axis=1)\n",
    "\n",
    "total_data['created_utc'] = pd.to_datetime(total_data['created_utc'], unit='s')\n",
    "total_data['Media']=total_data['Media'].notna().astype(int)\n",
    "total_data['reward_model']=list(reward['reward_model'])\n",
    "total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensor(input_str):\n",
    "    cleaned_str = input_str.strip().replace(\"tensor(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "    cleaned_str = cleaned_str.replace(\"\\n\", \"\").replace(\" \", \"\")\n",
    "    numbers = [float(num) for num in cleaned_str.split(\",\") if num]\n",
    "    return torch.tensor(numbers)\n",
    "\n",
    "def expand_vector_column(df, column_name, prefix):\n",
    "    vectors = np.vstack(df[column_name])\n",
    "    vector_df = pd.DataFrame(vectors, columns=[f\"{prefix}_{i}\" for i in range(vectors.shape[1])]) \n",
    "    return vector_df\n",
    "\n",
    "total_data['distilbert'] =total_data['distilbert'].astype(str)\n",
    "total_data['distilbert'] = total_data['distilbert'].apply(convert_to_tensor)\n",
    "bert=expand_vector_column(total_data, 'distilbert', 'bert')    \n",
    "\n",
    "total_data = pd.concat([total_data.drop(columns=['distilbert']), bert], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_total_data = all_data[all_data['created_utc'] < pd.to_datetime('2018-03-04')]\n",
    "post_total_data = all_data[all_data['created_utc'] >= pd.to_datetime('2018-03-04')]\n",
    "\n",
    "pre_total_data = pre_total_data.drop(columns=['created_utc'])\n",
    "post_total_data = post_total_data.drop(columns=['created_utc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_total_data.to_csv('c:\\\\Users\\\\aless\\\\Downloads\\\\data_pre_reward_x_final_classification.csv')\n",
    "post_total_data.to_csv('c:\\\\Users\\\\aless\\\\Downloads\\\\data_post_reward_x_final_classification.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Up-Down</th>\n",
       "      <th>Num_Comments</th>\n",
       "      <th>Media</th>\n",
       "      <th>flair_adoption</th>\n",
       "      <th>flair_altcoin</th>\n",
       "      <th>flair_announcement</th>\n",
       "      <th>flair_comedy</th>\n",
       "      <th>flair_dapp</th>\n",
       "      <th>flair_discussion</th>\n",
       "      <th>...</th>\n",
       "      <th>bert_758</th>\n",
       "      <th>bert_759</th>\n",
       "      <th>bert_760</th>\n",
       "      <th>bert_761</th>\n",
       "      <th>bert_762</th>\n",
       "      <th>bert_763</th>\n",
       "      <th>bert_764</th>\n",
       "      <th>bert_765</th>\n",
       "      <th>bert_766</th>\n",
       "      <th>bert_767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.4693</td>\n",
       "      <td>-0.71732</td>\n",
       "      <td>-1.11810</td>\n",
       "      <td>1.05830</td>\n",
       "      <td>-1.3588</td>\n",
       "      <td>0.388220</td>\n",
       "      <td>-2.0808</td>\n",
       "      <td>-0.84124</td>\n",
       "      <td>-0.498630</td>\n",
       "      <td>-0.78077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.4375</td>\n",
       "      <td>-0.33521</td>\n",
       "      <td>-0.98417</td>\n",
       "      <td>0.60537</td>\n",
       "      <td>-1.3027</td>\n",
       "      <td>-0.005083</td>\n",
       "      <td>-1.9859</td>\n",
       "      <td>-0.75455</td>\n",
       "      <td>-0.063954</td>\n",
       "      <td>-0.88743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.7170</td>\n",
       "      <td>-0.36867</td>\n",
       "      <td>-0.92320</td>\n",
       "      <td>0.87915</td>\n",
       "      <td>-1.3446</td>\n",
       "      <td>-0.109510</td>\n",
       "      <td>-1.7503</td>\n",
       "      <td>-0.49715</td>\n",
       "      <td>-0.507740</td>\n",
       "      <td>-0.85654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.2256</td>\n",
       "      <td>-0.58304</td>\n",
       "      <td>-1.04297</td>\n",
       "      <td>0.90395</td>\n",
       "      <td>-1.0266</td>\n",
       "      <td>0.095971</td>\n",
       "      <td>-1.8184</td>\n",
       "      <td>-0.52708</td>\n",
       "      <td>-0.474220</td>\n",
       "      <td>-0.35750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.4307</td>\n",
       "      <td>-0.48110</td>\n",
       "      <td>-0.73013</td>\n",
       "      <td>0.77413</td>\n",
       "      <td>-1.0524</td>\n",
       "      <td>-0.086142</td>\n",
       "      <td>-1.8239</td>\n",
       "      <td>-0.80496</td>\n",
       "      <td>-0.171310</td>\n",
       "      <td>-0.37328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65690</th>\n",
       "      <td>1690</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.6319</td>\n",
       "      <td>-0.45458</td>\n",
       "      <td>-0.37450</td>\n",
       "      <td>0.79077</td>\n",
       "      <td>-1.3192</td>\n",
       "      <td>0.251540</td>\n",
       "      <td>-1.4710</td>\n",
       "      <td>-0.75592</td>\n",
       "      <td>-0.331730</td>\n",
       "      <td>-0.46270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65691</th>\n",
       "      <td>1691</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.6900</td>\n",
       "      <td>-0.56871</td>\n",
       "      <td>-0.51279</td>\n",
       "      <td>0.96698</td>\n",
       "      <td>-1.6035</td>\n",
       "      <td>0.041926</td>\n",
       "      <td>-1.6938</td>\n",
       "      <td>-0.54013</td>\n",
       "      <td>-0.327800</td>\n",
       "      <td>-1.17990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65692</th>\n",
       "      <td>1692</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.5386</td>\n",
       "      <td>-0.85031</td>\n",
       "      <td>-0.28386</td>\n",
       "      <td>0.90991</td>\n",
       "      <td>-1.3002</td>\n",
       "      <td>0.056882</td>\n",
       "      <td>-1.7130</td>\n",
       "      <td>-0.39817</td>\n",
       "      <td>-0.341450</td>\n",
       "      <td>-0.82690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65693</th>\n",
       "      <td>1693</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.8290</td>\n",
       "      <td>-0.67560</td>\n",
       "      <td>-0.40747</td>\n",
       "      <td>0.89618</td>\n",
       "      <td>-1.1387</td>\n",
       "      <td>0.051005</td>\n",
       "      <td>-1.8844</td>\n",
       "      <td>-0.41055</td>\n",
       "      <td>-0.140060</td>\n",
       "      <td>-0.75554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65694</th>\n",
       "      <td>1694</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.6239</td>\n",
       "      <td>-0.62610</td>\n",
       "      <td>-0.30243</td>\n",
       "      <td>0.83220</td>\n",
       "      <td>-1.3763</td>\n",
       "      <td>0.248490</td>\n",
       "      <td>-1.8705</td>\n",
       "      <td>-0.41207</td>\n",
       "      <td>-0.382470</td>\n",
       "      <td>-0.87724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65695 rows × 820 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  Up-Down  Num_Comments  Media  flair_adoption  \\\n",
       "0               0       10             9      0               0   \n",
       "1               1       12            27      0               0   \n",
       "2               2        5             7      0               0   \n",
       "3               3        3             2      0               0   \n",
       "4               4        9            11      0               0   \n",
       "...           ...      ...           ...    ...             ...   \n",
       "65690        1690        2             2      0               0   \n",
       "65691        1691       16            23      0               0   \n",
       "65692        1692        2             0      0               0   \n",
       "65693        1693        1            12      0               0   \n",
       "65694        1694        3             2      0               0   \n",
       "\n",
       "       flair_altcoin  flair_announcement  flair_comedy  flair_dapp  \\\n",
       "0                  0                   0             0           0   \n",
       "1                  0                   0             0           0   \n",
       "2                  0                   0             0           0   \n",
       "3                  0                   0             0           0   \n",
       "4                  0                   0             0           0   \n",
       "...              ...                 ...           ...         ...   \n",
       "65690              0                   0             0           0   \n",
       "65691              0                   0             0           0   \n",
       "65692              0                   0             0           0   \n",
       "65693              0                   0             0           0   \n",
       "65694              0                   0             0           0   \n",
       "\n",
       "       flair_discussion  ...  bert_758  bert_759  bert_760  bert_761  \\\n",
       "0                     0  ...   -2.4693  -0.71732  -1.11810   1.05830   \n",
       "1                     0  ...   -2.4375  -0.33521  -0.98417   0.60537   \n",
       "2                     0  ...   -2.7170  -0.36867  -0.92320   0.87915   \n",
       "3                     0  ...   -2.2256  -0.58304  -1.04297   0.90395   \n",
       "4                     0  ...   -2.4307  -0.48110  -0.73013   0.77413   \n",
       "...                 ...  ...       ...       ...       ...       ...   \n",
       "65690                 0  ...   -2.6319  -0.45458  -0.37450   0.79077   \n",
       "65691                 0  ...   -2.6900  -0.56871  -0.51279   0.96698   \n",
       "65692                 0  ...   -2.5386  -0.85031  -0.28386   0.90991   \n",
       "65693                 0  ...   -2.8290  -0.67560  -0.40747   0.89618   \n",
       "65694                 0  ...   -2.6239  -0.62610  -0.30243   0.83220   \n",
       "\n",
       "       bert_762  bert_763  bert_764  bert_765  bert_766  bert_767  \n",
       "0       -1.3588  0.388220   -2.0808  -0.84124 -0.498630  -0.78077  \n",
       "1       -1.3027 -0.005083   -1.9859  -0.75455 -0.063954  -0.88743  \n",
       "2       -1.3446 -0.109510   -1.7503  -0.49715 -0.507740  -0.85654  \n",
       "3       -1.0266  0.095971   -1.8184  -0.52708 -0.474220  -0.35750  \n",
       "4       -1.0524 -0.086142   -1.8239  -0.80496 -0.171310  -0.37328  \n",
       "...         ...       ...       ...       ...       ...       ...  \n",
       "65690   -1.3192  0.251540   -1.4710  -0.75592 -0.331730  -0.46270  \n",
       "65691   -1.6035  0.041926   -1.6938  -0.54013 -0.327800  -1.17990  \n",
       "65692   -1.3002  0.056882   -1.7130  -0.39817 -0.341450  -0.82690  \n",
       "65693   -1.1387  0.051005   -1.8844  -0.41055 -0.140060  -0.75554  \n",
       "65694   -1.3763  0.248490   -1.8705  -0.41207 -0.382470  -0.87724  \n",
       "\n",
       "[65695 rows x 820 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_total_data= pd.read_csv('insert path')      #'c:\\\\Users\\\\aless\\\\Downloads\\\\data_pre_reward_x_final_classification.csv'\n",
    "pre_total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Up-Down</th>\n",
       "      <th>Num_Comments</th>\n",
       "      <th>Media</th>\n",
       "      <th>flair_adoption</th>\n",
       "      <th>flair_announcement</th>\n",
       "      <th>flair_comedy</th>\n",
       "      <th>flair_dapp</th>\n",
       "      <th>flair_discussion</th>\n",
       "      <th>flair_educational</th>\n",
       "      <th>...</th>\n",
       "      <th>bert_758</th>\n",
       "      <th>bert_759</th>\n",
       "      <th>bert_760</th>\n",
       "      <th>bert_761</th>\n",
       "      <th>bert_762</th>\n",
       "      <th>bert_763</th>\n",
       "      <th>bert_764</th>\n",
       "      <th>bert_765</th>\n",
       "      <th>bert_766</th>\n",
       "      <th>bert_767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.1102</td>\n",
       "      <td>0.69660</td>\n",
       "      <td>0.32780</td>\n",
       "      <td>0.148700</td>\n",
       "      <td>1.9013</td>\n",
       "      <td>-1.21450</td>\n",
       "      <td>0.083700</td>\n",
       "      <td>-0.70660</td>\n",
       "      <td>1.03970</td>\n",
       "      <td>0.70790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0213</td>\n",
       "      <td>0.55942</td>\n",
       "      <td>0.14586</td>\n",
       "      <td>0.059950</td>\n",
       "      <td>1.9020</td>\n",
       "      <td>-0.88164</td>\n",
       "      <td>-0.091699</td>\n",
       "      <td>-0.55292</td>\n",
       "      <td>0.70170</td>\n",
       "      <td>1.12980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0739</td>\n",
       "      <td>0.76920</td>\n",
       "      <td>0.66532</td>\n",
       "      <td>0.111300</td>\n",
       "      <td>2.0999</td>\n",
       "      <td>-1.16340</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>-0.65240</td>\n",
       "      <td>0.97220</td>\n",
       "      <td>1.06120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>124</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0176</td>\n",
       "      <td>0.94402</td>\n",
       "      <td>0.41523</td>\n",
       "      <td>0.256580</td>\n",
       "      <td>1.9387</td>\n",
       "      <td>-0.93738</td>\n",
       "      <td>-0.049934</td>\n",
       "      <td>-0.42100</td>\n",
       "      <td>0.89963</td>\n",
       "      <td>0.98649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.5878</td>\n",
       "      <td>0.68683</td>\n",
       "      <td>0.18753</td>\n",
       "      <td>0.059817</td>\n",
       "      <td>2.0965</td>\n",
       "      <td>-0.95004</td>\n",
       "      <td>0.421650</td>\n",
       "      <td>-0.78431</td>\n",
       "      <td>0.63633</td>\n",
       "      <td>0.89736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248315</th>\n",
       "      <td>13259</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.9638</td>\n",
       "      <td>1.14360</td>\n",
       "      <td>2.58440</td>\n",
       "      <td>0.092437</td>\n",
       "      <td>2.0163</td>\n",
       "      <td>-0.90838</td>\n",
       "      <td>0.326670</td>\n",
       "      <td>-0.68376</td>\n",
       "      <td>0.51854</td>\n",
       "      <td>1.16360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248316</th>\n",
       "      <td>13260</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0271</td>\n",
       "      <td>0.77190</td>\n",
       "      <td>2.84336</td>\n",
       "      <td>0.286800</td>\n",
       "      <td>1.9576</td>\n",
       "      <td>-1.09590</td>\n",
       "      <td>0.138900</td>\n",
       "      <td>-0.68030</td>\n",
       "      <td>0.95610</td>\n",
       "      <td>1.00070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248317</th>\n",
       "      <td>13261</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0141</td>\n",
       "      <td>0.68621</td>\n",
       "      <td>3.12756</td>\n",
       "      <td>0.157220</td>\n",
       "      <td>1.7806</td>\n",
       "      <td>-0.99189</td>\n",
       "      <td>0.080964</td>\n",
       "      <td>-0.35149</td>\n",
       "      <td>0.71987</td>\n",
       "      <td>1.22440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248318</th>\n",
       "      <td>13262</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.1588</td>\n",
       "      <td>0.89868</td>\n",
       "      <td>2.71220</td>\n",
       "      <td>0.039124</td>\n",
       "      <td>1.8470</td>\n",
       "      <td>-0.96300</td>\n",
       "      <td>0.001657</td>\n",
       "      <td>-0.54827</td>\n",
       "      <td>0.96191</td>\n",
       "      <td>1.03810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248319</th>\n",
       "      <td>13263</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.9923</td>\n",
       "      <td>0.71954</td>\n",
       "      <td>3.01087</td>\n",
       "      <td>0.252250</td>\n",
       "      <td>1.9724</td>\n",
       "      <td>-0.91599</td>\n",
       "      <td>0.165600</td>\n",
       "      <td>-0.74849</td>\n",
       "      <td>0.65720</td>\n",
       "      <td>1.25320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>248320 rows × 820 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  Up-Down  Num_Comments  Media  flair_adoption  \\\n",
       "0                0        2             6      0               0   \n",
       "1                1        1             8      0               0   \n",
       "2                2        5             8      1               0   \n",
       "3                3      124            21      1               0   \n",
       "4                4       20             4      0               0   \n",
       "...            ...      ...           ...    ...             ...   \n",
       "248315       13259        1             0      0               0   \n",
       "248316       13260        1             0      0               0   \n",
       "248317       13261        1             4      0               0   \n",
       "248318       13262        1             0      0               0   \n",
       "248319       13263        1             0      0               0   \n",
       "\n",
       "        flair_announcement  flair_comedy  flair_dapp  flair_discussion  \\\n",
       "0                        0             0           0                 0   \n",
       "1                        0             0           0                 0   \n",
       "2                        0             0           0                 0   \n",
       "3                        0             0           0                 0   \n",
       "4                        0             1           0                 0   \n",
       "...                    ...           ...         ...               ...   \n",
       "248315                   0             0           0                 0   \n",
       "248316                   0             0           0                 0   \n",
       "248317                   0             0           0                 0   \n",
       "248318                   0             0           0                 0   \n",
       "248319                   0             0           0                 0   \n",
       "\n",
       "        flair_educational  ...  bert_758  bert_759  bert_760  bert_761  \\\n",
       "0                       0  ...   -2.1102   0.69660   0.32780  0.148700   \n",
       "1                       0  ...   -2.0213   0.55942   0.14586  0.059950   \n",
       "2                       0  ...   -2.0739   0.76920   0.66532  0.111300   \n",
       "3                       0  ...   -2.0176   0.94402   0.41523  0.256580   \n",
       "4                       0  ...   -1.5878   0.68683   0.18753  0.059817   \n",
       "...                   ...  ...       ...       ...       ...       ...   \n",
       "248315                  0  ...   -1.9638   1.14360   2.58440  0.092437   \n",
       "248316                  0  ...   -2.0271   0.77190   2.84336  0.286800   \n",
       "248317                  0  ...   -2.0141   0.68621   3.12756  0.157220   \n",
       "248318                  0  ...   -2.1588   0.89868   2.71220  0.039124   \n",
       "248319                  0  ...   -1.9923   0.71954   3.01087  0.252250   \n",
       "\n",
       "        bert_762  bert_763  bert_764  bert_765  bert_766  bert_767  \n",
       "0         1.9013  -1.21450  0.083700  -0.70660   1.03970   0.70790  \n",
       "1         1.9020  -0.88164 -0.091699  -0.55292   0.70170   1.12980  \n",
       "2         2.0999  -1.16340  0.064000  -0.65240   0.97220   1.06120  \n",
       "3         1.9387  -0.93738 -0.049934  -0.42100   0.89963   0.98649  \n",
       "4         2.0965  -0.95004  0.421650  -0.78431   0.63633   0.89736  \n",
       "...          ...       ...       ...       ...       ...       ...  \n",
       "248315    2.0163  -0.90838  0.326670  -0.68376   0.51854   1.16360  \n",
       "248316    1.9576  -1.09590  0.138900  -0.68030   0.95610   1.00070  \n",
       "248317    1.7806  -0.99189  0.080964  -0.35149   0.71987   1.22440  \n",
       "248318    1.8470  -0.96300  0.001657  -0.54827   0.96191   1.03810  \n",
       "248319    1.9724  -0.91599  0.165600  -0.74849   0.65720   1.25320  \n",
       "\n",
       "[248320 rows x 820 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_total_data= pd.read_csv('insert path')    #'c:\\\\Users\\\\aless\\\\Downloads\\\\data_post_reward_x_final_classification.csv\n",
    "post_total_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
